{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesion_vgg16_non-aug_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SmUmmSr6RF9",
        "outputId": "f4d5bba9-a0d9-4150-d208-7db9d63c54d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 24 13:46:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrDSYQ0jhn0A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import keras\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from sys import getsizeof\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import f1_score\n",
        "from skimage.transform import rescale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "HzCcCp9moaF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_path = '/content/drive/MyDrive/Newcastle University/Machine Learning/lesions'"
      ],
      "metadata": {
        "id": "IOMk8f_dmKwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def load_paths(path):\n",
        "  \n",
        "  image_paths = []\n",
        "  y = []\n",
        "  \n",
        "  for label, elem in enumerate(os.listdir(path)):\n",
        "\n",
        "    print('class: ' + elem + ' | label: ' + str(label))\n",
        "    path1 = path + '/' + elem\n",
        "    images = os.listdir(path1)\n",
        "    for im in images:\n",
        "      image_paths.append(path1 + '/' + im)\n",
        "      y.append(str(label))\n",
        "  \n",
        "  # shuffle\n",
        "  c = list(zip(image_paths,y))\n",
        "  np.random.shuffle(c)\n",
        "  image_paths,y = zip(*c)\n",
        "  \n",
        "  return image_paths, keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "image_paths, labels = load_paths(data_path)"
      ],
      "metadata": {
        "id": "M_-pKhKXxIXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(image_paths, labels, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "RslIdZEqxIap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data (non-augmented)\n",
        "train_path1 = '/content/drive/MyDrive/NCL/ML/Dataset/train'\n",
        "# Training data (augmented)\n",
        "train_path2 = '/content/drive/MyDrive/NCL/ML/Dataset/output'\n",
        "# Testing data\n",
        "valid_path = '/content/drive/MyDrive/NCL/ML/Dataset/validation'"
      ],
      "metadata": {
        "id": "AFCQOVwsqPFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = sorted(os.listdir(train_path2))"
      ],
      "metadata": {
        "id": "Lkn46_DxLmhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = []\n",
        "train_labels = []\n",
        "\n",
        "#elem_list1 = os.listdir(path_train)\n",
        "\n",
        "for label, elem in enumerate(classes):\n",
        "  \n",
        "  path1 = train_path1 + '/' + str(elem)\n",
        "  images = os.listdir(path1)\n",
        "  \n",
        "  for im in images:\n",
        "      \n",
        "      # takes the whole path of the images\n",
        "      # removes the need to mention the dataset path for the generator\n",
        "      train_list.append(path1 + '/' + str(im))\n",
        "      train_labels.append(label)\n",
        "\n",
        "# shuffling the image list\n",
        "c = list(zip(train_list, train_labels))\n",
        "np.random.shuffle(c)\n",
        "train_list, train_labels = zip(*c)\n",
        "\n",
        "# one hot encoding\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes=7)"
      ],
      "metadata": {
        "id": "ilEC-AuZLomZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = []\n",
        "test_labels = []\n",
        "\n",
        "#elem_list1 = os.listdir(path_train)\n",
        "\n",
        "for label, elem in enumerate(classes):\n",
        "  \n",
        "  path1 = valid_path + '/' + str(elem)\n",
        "  images = os.listdir(path1)\n",
        "  \n",
        "  for im in images:\n",
        "      \n",
        "      # takes the whole path of the images\n",
        "      # removes the need to mention the dataset path for the generator\n",
        "      test_list.append(path1 + '/' + str(im))\n",
        "      test_labels.append(label)\n",
        "\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes=7)"
      ],
      "metadata": {
        "id": "7R2woQJ5MBEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX_Npcuere-o",
        "outputId": "7aa06e2d-b850-4706-9513-e65fee1ec625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8010, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-vDMM_OrKI6",
        "outputId": "a539dc50-09b3-4441-cf45-b797662bd7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2005, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator for data loading while training the model\n",
        "def im_datagen(image_list, label_list, batch_size = 32, shuffle = False):\n",
        "    while True:\n",
        "        # shuffle the data\n",
        "        if shuffle == True:\n",
        "            c = list(zip(image_list, label_list))\n",
        "            np.random.shuffle(c)\n",
        "            image_list, label_list = zip(*c)\n",
        "    \n",
        "        total_elements = len(label_list)\n",
        "    \n",
        "        for i in range(0, total_elements, batch_size):\n",
        "          \n",
        "          images = [cv2.resize(cv2.imread(x), (224,224)) for x in image_list[i:i+batch_size]]\n",
        "          images = np.asarray(images)\n",
        "          images = images/255.\n",
        "          \n",
        "          yield images, keras.utils.to_categorical(label_list[i:i+batch_size], num_classes = 7)"
      ],
      "metadata": {
        "id": "r71nYx7-Lopc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size = 50"
      ],
      "metadata": {
        "id": "v9HScY7I3932"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP_SIZE_TRAIN = len(train_labels)//batch_size\n",
        "STEP_SIZE_VALID = len(test_labels)//batch_size\n",
        "\n",
        "print(STEP_SIZE_TRAIN)\n",
        "print(STEP_SIZE_VALID)"
      ],
      "metadata": {
        "id": "8cmqcVB3cY6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_gen = im_datagen(train_list, train_labels, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "test_gen = im_datagen(test_list, test_labels, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "mxMglYp_dZas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_test.[17]"
      ],
      "metadata": {
        "id": "DnBt86wk4t3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "y_test[17]"
      ],
      "metadata": {
        "id": "yxN19TlH8P4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter(y_test).keys() # equals to list(set(words))"
      ],
      "metadata": {
        "id": "COF20gK3xIeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter(y_test).values() # counts the elements' frequency"
      ],
      "metadata": {
        "id": "antHWObT3nF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeisonSequence(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size = 32):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.x) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "\n",
        "        images = []\n",
        "\n",
        "        for file_name in batch_x:\n",
        "          images.append(cv2.resize(cv2.imread(file_name), (224, 224)))\n",
        "\n",
        "        return (np.array(images)/255), np.array(batch_y)"
      ],
      "metadata": {
        "id": "qLIkj9LCxIhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "test = []\n",
        "\n",
        "for file_name in x_test[0:5]:\n",
        "  test.append(rescale(io.imread(file_name), (0.5, 0.5, 1)))\n",
        "  #print(file_name)\n",
        "\n",
        "test = np.array(test)\n",
        "plt.imshow(test[3])"
      ],
      "metadata": {
        "id": "AWR3pGgIfO_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_gen = LeisonSequence(train_list, train_labels, 100)\n",
        "val_img_gen = LeisonSequence(test_list, test_labels, 50)"
      ],
      "metadata": {
        "id": "x-EVrrT2xIl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base_model = keras.applications.ResNet50(\n",
        "model = keras.applications.VGG16(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=(224, 224, 3),\n",
        "    classes=7,\n",
        "    pooling=None\n",
        ")"
      ],
      "metadata": {
        "id": "o18kmISZ5Zfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base_model.summary()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aYNxMbg6RJD",
        "outputId": "2a48bb1a-3413-4292-f430-e44816e75bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 7)                 28679     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134,289,223\n",
            "Trainable params: 134,289,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "last_layer = base_model.get_layer('global_average_pooling2d')\n",
        "\n",
        "print ('last layer output shape:', last_layer.output_shape)\n",
        "\n",
        "bm_out = last_layer.output\n",
        "#x = base_model(inputs, training=False)\n",
        "x = keras.layers.Dense(128, activation='relu')(bm_out)\n",
        "#x = keras.layers.Dropout(0.5)(x)\n",
        "outputs = keras.layers.Dense(7, activation='softmax')(x)\n",
        "#model = keras.Model(inputs, outputs)\n",
        "model = keras.Model(base_model.input, outputs)"
      ],
      "metadata": {
        "id": "drtS5_zLyPjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "UvClYg6czAlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0002,\n",
        "    decay_steps=100,\n",
        "    decay_rate=0.9)\n",
        "sgd = keras.optimizers.SGD(learning_rate=0.0002)\n",
        "adam = keras.optimizers.Adam(learning_rate=lr_schedule, amsgrad=True)\n",
        "nadam = keras.optimizers.Nadam(learning_rate=0.0002)"
      ],
      "metadata": {
        "id": "x8q9amiXYVZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=sgd ,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KDeUMEV66RMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/NCL/ML/Models/vgg16_non-aug.h5')"
      ],
      "metadata": {
        "id": "yVvfUzC8x35n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp = keras.callbacks.ModelCheckpoint(filepath = '/content/drive/MyDrive/NCL/ML/Models/vgg16_non-aug_2.h5', verbose = 1, save_best_only = True, monitor='val_accuracy')"
      ],
      "metadata": {
        "id": "XFcQU2BP6RPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_img_gen, epochs=50, validation_data=val_img_gen, callbacks=cp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b8WWMNB6RR_",
        "outputId": "301d88b1-9dc0-404b-f724-c3d2775be712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8508 \n",
            "Epoch 00001: val_accuracy improved from -inf to 0.74863, saving model to /content/drive/MyDrive/NCL/ML/Models/vgg16_non-aug_2.h5\n",
            "81/81 [==============================] - 2128s 26s/step - loss: 0.4032 - accuracy: 0.8508 - val_loss: 0.7532 - val_accuracy: 0.7486\n",
            "Epoch 2/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3888 - accuracy: 0.8581\n",
            "Epoch 00002: val_accuracy improved from 0.74863 to 0.75561, saving model to /content/drive/MyDrive/NCL/ML/Models/vgg16_non-aug_2.h5\n",
            "81/81 [==============================] - 167s 2s/step - loss: 0.3888 - accuracy: 0.8581 - val_loss: 0.7747 - val_accuracy: 0.7556\n",
            "Epoch 3/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8619\n",
            "Epoch 00003: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 164s 2s/step - loss: 0.3771 - accuracy: 0.8619 - val_loss: 0.7804 - val_accuracy: 0.7551\n",
            "Epoch 4/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8644\n",
            "Epoch 00004: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3759 - accuracy: 0.8644 - val_loss: 0.7905 - val_accuracy: 0.7496\n",
            "Epoch 5/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8660\n",
            "Epoch 00005: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3685 - accuracy: 0.8660 - val_loss: 0.7962 - val_accuracy: 0.7491\n",
            "Epoch 6/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8649\n",
            "Epoch 00006: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3691 - accuracy: 0.8649 - val_loss: 0.7937 - val_accuracy: 0.7541\n",
            "Epoch 7/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8674\n",
            "Epoch 00007: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3599 - accuracy: 0.8674 - val_loss: 0.8053 - val_accuracy: 0.7531\n",
            "Epoch 8/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.8665\n",
            "Epoch 00008: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3693 - accuracy: 0.8665 - val_loss: 0.8140 - val_accuracy: 0.7506\n",
            "Epoch 9/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8669\n",
            "Epoch 00009: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3621 - accuracy: 0.8669 - val_loss: 0.8148 - val_accuracy: 0.7521\n",
            "Epoch 10/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.8688\n",
            "Epoch 00010: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3643 - accuracy: 0.8688 - val_loss: 0.8657 - val_accuracy: 0.7456\n",
            "Epoch 11/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8699\n",
            "Epoch 00011: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3568 - accuracy: 0.8699 - val_loss: 0.8336 - val_accuracy: 0.7476\n",
            "Epoch 12/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.8745\n",
            "Epoch 00012: val_accuracy did not improve from 0.75561\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3521 - accuracy: 0.8745 - val_loss: 0.8431 - val_accuracy: 0.7526\n",
            "Epoch 13/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8723\n",
            "Epoch 00013: val_accuracy improved from 0.75561 to 0.75661, saving model to /content/drive/MyDrive/NCL/ML/Models/vgg16_non-aug_2.h5\n",
            "81/81 [==============================] - 164s 2s/step - loss: 0.3557 - accuracy: 0.8723 - val_loss: 0.8167 - val_accuracy: 0.7566\n",
            "Epoch 14/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8705\n",
            "Epoch 00014: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 161s 2s/step - loss: 0.3544 - accuracy: 0.8705 - val_loss: 0.8151 - val_accuracy: 0.7501\n",
            "Epoch 15/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.8732\n",
            "Epoch 00015: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3510 - accuracy: 0.8732 - val_loss: 0.8445 - val_accuracy: 0.7546\n",
            "Epoch 16/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8720\n",
            "Epoch 00016: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3547 - accuracy: 0.8720 - val_loss: 0.8325 - val_accuracy: 0.7526\n",
            "Epoch 17/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8719\n",
            "Epoch 00017: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3498 - accuracy: 0.8719 - val_loss: 0.8345 - val_accuracy: 0.7526\n",
            "Epoch 18/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8750\n",
            "Epoch 00018: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3466 - accuracy: 0.8750 - val_loss: 0.8745 - val_accuracy: 0.7481\n",
            "Epoch 19/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8749\n",
            "Epoch 00019: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3454 - accuracy: 0.8749 - val_loss: 0.8424 - val_accuracy: 0.7476\n",
            "Epoch 20/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8740\n",
            "Epoch 00020: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3469 - accuracy: 0.8740 - val_loss: 0.8566 - val_accuracy: 0.7506\n",
            "Epoch 21/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8734\n",
            "Epoch 00021: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3502 - accuracy: 0.8734 - val_loss: 0.8355 - val_accuracy: 0.7491\n",
            "Epoch 22/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8763\n",
            "Epoch 00022: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3443 - accuracy: 0.8763 - val_loss: 0.8467 - val_accuracy: 0.7481\n",
            "Epoch 23/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8765\n",
            "Epoch 00023: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3421 - accuracy: 0.8765 - val_loss: 0.8950 - val_accuracy: 0.7516\n",
            "Epoch 24/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.8757\n",
            "Epoch 00024: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3394 - accuracy: 0.8757 - val_loss: 0.8730 - val_accuracy: 0.7516\n",
            "Epoch 25/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.8780\n",
            "Epoch 00025: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3394 - accuracy: 0.8780 - val_loss: 0.8454 - val_accuracy: 0.7521\n",
            "Epoch 26/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8792\n",
            "Epoch 00026: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3404 - accuracy: 0.8792 - val_loss: 0.8458 - val_accuracy: 0.7501\n",
            "Epoch 27/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8757\n",
            "Epoch 00027: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3404 - accuracy: 0.8757 - val_loss: 0.8628 - val_accuracy: 0.7486\n",
            "Epoch 28/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8799\n",
            "Epoch 00028: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3328 - accuracy: 0.8799 - val_loss: 0.8665 - val_accuracy: 0.7511\n",
            "Epoch 29/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8814\n",
            "Epoch 00029: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3330 - accuracy: 0.8814 - val_loss: 0.8314 - val_accuracy: 0.7486\n",
            "Epoch 30/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8810\n",
            "Epoch 00030: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3323 - accuracy: 0.8810 - val_loss: 0.8667 - val_accuracy: 0.7516\n",
            "Epoch 31/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8794\n",
            "Epoch 00031: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3328 - accuracy: 0.8794 - val_loss: 0.8633 - val_accuracy: 0.7461\n",
            "Epoch 32/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.8772\n",
            "Epoch 00032: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3385 - accuracy: 0.8772 - val_loss: 0.8626 - val_accuracy: 0.7481\n",
            "Epoch 33/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3273 - accuracy: 0.8834\n",
            "Epoch 00033: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 158s 2s/step - loss: 0.3273 - accuracy: 0.8834 - val_loss: 0.8673 - val_accuracy: 0.7416\n",
            "Epoch 34/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8814\n",
            "Epoch 00034: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3318 - accuracy: 0.8814 - val_loss: 0.8634 - val_accuracy: 0.7471\n",
            "Epoch 35/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8821\n",
            "Epoch 00035: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3305 - accuracy: 0.8821 - val_loss: 0.8918 - val_accuracy: 0.7531\n",
            "Epoch 36/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.8811\n",
            "Epoch 00036: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3292 - accuracy: 0.8811 - val_loss: 0.8767 - val_accuracy: 0.7501\n",
            "Epoch 37/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.8841\n",
            "Epoch 00037: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3272 - accuracy: 0.8841 - val_loss: 0.8731 - val_accuracy: 0.7461\n",
            "Epoch 38/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.8825\n",
            "Epoch 00038: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3284 - accuracy: 0.8825 - val_loss: 0.8761 - val_accuracy: 0.7511\n",
            "Epoch 39/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.8811\n",
            "Epoch 00039: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 158s 2s/step - loss: 0.3298 - accuracy: 0.8811 - val_loss: 0.9238 - val_accuracy: 0.7471\n",
            "Epoch 40/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8840\n",
            "Epoch 00040: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3226 - accuracy: 0.8840 - val_loss: 0.8898 - val_accuracy: 0.7411\n",
            "Epoch 41/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.8839\n",
            "Epoch 00041: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3269 - accuracy: 0.8839 - val_loss: 0.8690 - val_accuracy: 0.7486\n",
            "Epoch 42/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.8824\n",
            "Epoch 00042: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 158s 2s/step - loss: 0.3277 - accuracy: 0.8824 - val_loss: 0.8737 - val_accuracy: 0.7506\n",
            "Epoch 43/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8846\n",
            "Epoch 00043: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3233 - accuracy: 0.8846 - val_loss: 0.8854 - val_accuracy: 0.7446\n",
            "Epoch 44/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.8821\n",
            "Epoch 00044: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3261 - accuracy: 0.8821 - val_loss: 0.8926 - val_accuracy: 0.7491\n",
            "Epoch 45/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8856\n",
            "Epoch 00045: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3224 - accuracy: 0.8856 - val_loss: 0.8795 - val_accuracy: 0.7521\n",
            "Epoch 46/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8850\n",
            "Epoch 00046: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3237 - accuracy: 0.8850 - val_loss: 0.8731 - val_accuracy: 0.7426\n",
            "Epoch 47/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8866\n",
            "Epoch 00047: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3206 - accuracy: 0.8866 - val_loss: 0.8809 - val_accuracy: 0.7521\n",
            "Epoch 48/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8869\n",
            "Epoch 00048: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 161s 2s/step - loss: 0.3209 - accuracy: 0.8869 - val_loss: 0.9022 - val_accuracy: 0.7496\n",
            "Epoch 49/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8848\n",
            "Epoch 00049: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 160s 2s/step - loss: 0.3234 - accuracy: 0.8848 - val_loss: 0.8843 - val_accuracy: 0.7466\n",
            "Epoch 50/50\n",
            "81/81 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.8846\n",
            "Epoch 00050: val_accuracy did not improve from 0.75661\n",
            "81/81 [==============================] - 159s 2s/step - loss: 0.3211 - accuracy: 0.8846 - val_loss: 0.8910 - val_accuracy: 0.7431\n"
          ]
        }
      ]
    }
  ]
}